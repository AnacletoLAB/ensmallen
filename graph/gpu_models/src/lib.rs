/// The path to the PTX generated by the GPU code
const PTX: &str = include_str!("../../cuda_kernels/target/nvptx64-nvidia-cuda/release/cuda_kernels.ptx");

mod wrappers;
use wrappers::*;

pub fn add_one(vals: &[usize]) -> Result<Vec<usize>, GPUError> {
    // get all the devices in the system
    let devices = Device::get_devices()?;
    // we use the first device
    let device = devices[0];

    // get info about this device
    let props = device.get_properties()?;
    println!("using GPU {}", device.get_name()?);
    let total_memory_mem = props.totalGlobalMem;

    // setup this device for computation
    let mut gpu = GPU::new(device)?;
    // load our compiled code
    let mut ptx = gpu.load_ptx(PTX)?;
    // get a function from the compiled code
    let kernel = ptx.get_kernel("test_kernel")?;
    
    // allocate a gpu buffer and copy data from the host
    let buffer = gpu.buffer_from_slice::<usize>(vals)?;

    // set the parallelizzation specs
    let grid = Grid::default().set_block_x(1024)?;

    // launch the function with the args
    gpu.launch_kernel(kernel, grid, args![
        buffer.as_device_ptr(),
        buffer.len(),
    ])?;
    // wait for the gpu to finish
    gpu.synchronize()?;

    // copy the results back from the gpu to the host
    let result = buffer.to_vec()?;
    Ok(result)
}